[
  {
    "objectID": "web_scraping.html",
    "href": "web_scraping.html",
    "title": "Web scraping",
    "section": "",
    "text": "loading data\nIn this example we will scrape the F1 driver data from wikipedia, that is online here. The large table contains a lot of data that we want to download. Again, we can not access the data from webR, that why we need to fake it a little, but the general idea on how to access a page online can be seen below.\n\nlibrary(rvest)\nlink &lt;- \"https://en.wikipedia.org/wiki/List_of_Formula_One_drivers\"\n\npage &lt;- read_html(link)\n\nWe take it interactively from here. First, we read in the html file. Above this, you can see how that would work using an url. The file has been downloaded to the VFS, but we can use the same command!\n\n\n\n\n\n\nNext, we need to find the right table. Navigate to wikipedia page and look for it. The easiest way to do this, is by right-clicking on the page and choosing inspect. You should see something like this.\n\nThe table is in an table element with the sortable attribute. The rvest package allow for functions that are prepared for this, so we just need to convert the raw page. You may have notices the clean_names() function.\n\n\ncleaning data\nThis comes from the janitor package, which contains very useful functions for data cleaning. It has been uploaded to your webR already, but check out the github link. Try to alter the code to see the effect of the package, you have all the tools available.\n\n\n\n\n\n\nNext, we are going to do some data cleaning. We only want to keep certain columns. To achieve this, we use the select() function from dplyr.\n\n\n\n\n\n\nAlso, please check out the last row of the dataframe. It appears that this is no data but an explanation of the columns. This would prevent using the data out-of-the-box. But we do not need it anyway, so we get rid of it. Check out the nrow() command by typing ?nrow to see what it does.\n\n\n\n\n\n\nIn the next step, we need to check out the column drivers_championship. Apparently, in the table are the number of championships the drivers one, but also the years. We are (for now) only interested in the number of won championships, so we get rid of the years information by simply extracting the first character using the substr() function. This is achieved by also using the mutate() command provided by dplyr, you can find more detailed references here.\n\n\n\n\n\n\nThe next step is there for safety reasons. We make sure with the parse_number() function from the readr package, that all numbers in the columns of interest are actual numbers.\n\n\n\n\n\n\nThis was a step by step intro, you could also use one long chain of command as outlined below.\n\ndrivers_F1 &lt;- drivers_F1 |&gt; \n  select(\n    driver_name,\n    nationality,\n    seasons_competed,\n    drivers_championships,\n    pole_positions,\n    race_wins,\n    podiums\n  ) |&gt; \n  slice_head(\n    n = nrow(drivers_F1)-1\n  ) |&gt; \n  mutate(\n    drivers_championships = parse_number(substr(drivers_championships,start = 1, stop = 1)),\n    race_wins = parse_number(race_wins),\n    pole_positions = parse_number(pole_positions)\n  )\n\n\n\nprocessing data\nSo, what now? We can look into how many championships by nation have been won. For this, we first group_by() nation, and then we summarise() the sum of all drivers_championships.\n\n\n\n\n\n\nThe output is… nothing. We need to look into the created object.\n\n\n\n\n\n\nBut the order is alphabetically, we want to see the top nations!\n\n\n\n\n\n\nOh no, wrong again, we want to have it descending.\n\n\n\n\n\n\nOk, and who is the driver with the most won championships?\n\n\n\n\n\n\n\n\nplotting data\nNext thing we want to know is, if there is a relationship between number of pole_positions and the number of championships that have been won by a driver (drivers_championsips).\n\n\n\n\n\n\nSo what happens in the code block? First we take the dataframe and filter it, so only driver_championships greater than 1 are retained. After that, we pipe it into ggplot2, a plotting system that is based on the Grammar of Graphics. Onto the x-axis we map the number of pole_positions and on the y-axis we map the number of driver_championships. This we plot with a geom_point(), so we tell the function to use points to represent the data. The position = jitter jitters the points and prevents overplotting. The labs() function assigns nice labels to the axis. The geom_smooth() function maps a linear regression line (including the standard error in light gray). In the end we can give the plot a nice theme with theme_minimal().",
    "crumbs": [
      "Data Sources",
      "Web scraping"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exercises for the Big Data lecture",
    "section": "",
    "text": "This is the landing page for the Big Data exercises taught by Prof. Dr. Tim Weber at the DIT. This is a more or less static page, for admin stuff got to ilearn. The exercises are designed to run in the browser of your choice. Feel free to setup R on your local machine, but do not expect support from us. You can download R here. You can download RStudio (the free IDE for R) here. If you want to get things going, start here. Packages forR can be downloaded from CRAN, but this should be done from within R. If you want to contribute or get packages that are not hosted on CRAN, you can setup a Github account, but this is purely optional.\n\nGet started\nData sources\n\n\n\n\nopenAI prompt: give me a picture about Big Data"
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "For BigData we need - Data. There are many sources out there, we will start working with a couple of them.\n\nFirst, we will look into data that is shipped with R.\nThen, we will check out how to work with csv files.\nAfter this, we will work on the mechanics of API calls.\nA quick introduction in working with databases is then given.\nAlso, we will have a short intro into web scraping\n\n\n\n\nopenAI prompt: give me a picture about Data Sources",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "csv_files.html",
    "href": "csv_files.html",
    "title": "csv files",
    "section": "",
    "text": "Reading a csv is fairly simple, we just need the file path. Lucky for you this has been prepared, the file was already downloaded to the Virtual File System (VFS) that webR uses. Classically, we can use the read.csv() function from baseR. It is not as convenient, but you do not need to install or download any package to work with it. Check out the variable content using the commands above!\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the tidyverse we also have the readr package. It give more flexibility, for example specifiying columns. We do not need to care about that now, but it is good to know.",
    "crumbs": [
      "Data Sources",
      "csv files"
    ]
  },
  {
    "objectID": "csv_files.html#dplyr",
    "href": "csv_files.html#dplyr",
    "title": "csv files",
    "section": "",
    "text": "In the tidyverse we also have the readr package. It give more flexibility, for example specifiying columns. We do not need to care about that now, but it is good to know.",
    "crumbs": [
      "Data Sources",
      "csv files"
    ]
  },
  {
    "objectID": "API_data.html",
    "href": "API_data.html",
    "title": "Calling API’s",
    "section": "",
    "text": "loading data\nBelow you can try to get data from an API. This is not yet possible using webR for technical and security reasons. In theory, it would go like this:\nThis is the Give Food API at givefood.org.uk and shall give us a brief introduction on how to work with API’s. We need two packages: httr and jsonlite. We are pulling data from a data source that does not need a key foodbank.\n\nlibrary(httr)\nlibrary(jsonlite)\n\nfoodbank &lt;- httr::GET(\"https://www.givefood.org.uk/api/2/foodbanks/\")\n\nfoodbankcontent &lt;- httr::content(foodbank, as = \"text\")\n\nfoodbankJSON &lt;- jsonlite::fromJSON(foodbankcontent)\n\nResponse [https://www.givefood.org.uk/api/2/foodbanks/]\n  Date: 2024-10-05 07:50\n  Status: 200\n  Content-Type: application/json\n  Size: 1.28 MB\n[\n  {\n    \"name\": \"Lapford Food Bank\",\n    \"alt_name\": null,\n    \"slug\": \"lapford\",\n    \"phone\": \"0136383788\",\n    \"secondary_phone\": null,\n    \"email\": \"foodbank@lapfordcc.org.uk\",\n    \"address\": \"Victory Hall\\r\\nLapford\\r\\nDevon\\r\\nEX17 6PZ\",\n    \"postcode\": \"EX17 6PZ\",\n...\nBut we can “fake” an API call, just to be able to play around a little with json files. For this we will download the data that is provided by the API as a csv and re-read it into R.\n\n\n\n\n\n\n\n\nprocessing data\nOk, so now we have data, but what can we do with it? Below is some code where we count the foodbanks according to region in the UK. The count() command is an abbreviation of the group_by() command and the summarise() command. It is quicker, but not as versatile. We use it here, to get some quick results.\n\n\n\n\n\n\n\n\nplotting data\nWe then add a ordered barplot so show the counts. Piece o’ cake!\n\n\n\n\n\n\nNotice that we have ordered the x-axis to start with the highest bar. This makes barplots much more readable as if we would not have done this.",
    "crumbs": [
      "Data Sources",
      "Calling API's"
    ]
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "Databases",
    "section": "",
    "text": "As with API’s a real connection to a remote database is right now not feasible using webR. But we can use duckDB which is a fast in-process analytical database. It has one more advantage: It is locally hosted, which means it is a physical file on your machine. We also use dbplyr which is a database backend that uses the same or similar logic as dplyr for data manipulation with remote data sources.",
    "crumbs": [
      "Data Sources",
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#play-around",
    "href": "databases.html#play-around",
    "title": "Databases",
    "section": "play around",
    "text": "play around\nYou can try to use ggplot on the data. The code below shall give you inspiration on how to start. Maybe you want to use the shape aesthetic?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nSee this hint on how to use the shape aesthetic and plot a rough linear model using geom_smooth().\nmtcars |&gt; \n  ggplot(\n    aes(\n      x = cyl,\n      y = hp,\n      shape = as.factor(gear)\n    )\n  )+\n  geom_point()+\n  geom_smooth(\n    method = \"lm\"\n  )",
    "crumbs": [
      "Data Sources",
      "Databases"
    ]
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Get started",
    "section": "",
    "text": "Below you can see R running in the browser. It is not as powerful as if we install R on the machine, but it spares us the hassle of setting R up on every of your Computers. You can type some simple math to try it out! You can run single lines by pressing Ctrl + Enter.",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#function-help",
    "href": "get_started.html#function-help",
    "title": "Get started",
    "section": "function help",
    "text": "function help",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#dataset-documentation",
    "href": "get_started.html#dataset-documentation",
    "title": "Get started",
    "section": "dataset documentation",
    "text": "dataset documentation",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#package-documentation",
    "href": "get_started.html#package-documentation",
    "title": "Get started",
    "section": "package documentation",
    "text": "package documentation",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#basic-piping",
    "href": "get_started.html#basic-piping",
    "title": "Get started",
    "section": "Basic piping",
    "text": "Basic piping\n\nx |&gt; f is equivalent to f(x)\nx |&gt; f(y) is equivalent to f(x,y)\nx |&gt; f |&gt; g |&gt; h is equivalent to h(g(f(x)))",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#examples",
    "href": "get_started.html#examples",
    "title": "Get started",
    "section": "examples",
    "text": "examples",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#base-r-reference",
    "href": "get_started.html#base-r-reference",
    "title": "Get started",
    "section": "Base R reference",
    "text": "Base R reference",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#package-citation",
    "href": "get_started.html#package-citation",
    "title": "Get started",
    "section": "package citation",
    "text": "package citation",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "get_started.html#footnotes",
    "href": "get_started.html#footnotes",
    "title": "Get started",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Working_directory↩︎",
    "crumbs": [
      "Get started"
    ]
  },
  {
    "objectID": "provided_data.html",
    "href": "provided_data.html",
    "title": "data shipped with R",
    "section": "",
    "text": "You can simply use data that is natively provided by R. A famous example is the titanic data set. Do this using the data() command as provided below\n\n\n\n\n\n\n\n\nThere is not a lot that happened, because we do not use an IDE. An IDE like RStudio would acutally display the variables. This is a trade-off we have to live with the convinienct of not setting up R, but of course, there are ways around it. Below are a couple of ways to display data: - the printcommand is the most verbose way to output data - the head command prints only the first couple of lines (can be specified using head(object, n = X) - the glimpse command is very convenient, but needs the dplyr package to be loaded or referenced (with :: as shown below) - the str command give the structure of the object, so it provides a meta view of the variable. This is convenient if you have some unexpected output.",
    "crumbs": [
      "Data Sources",
      "data shipped with R"
    ]
  },
  {
    "objectID": "provided_data.html#conversion-to-data-frame",
    "href": "provided_data.html#conversion-to-data-frame",
    "title": "data shipped with R",
    "section": "conversion to data frame",
    "text": "conversion to data frame\nYou can see, that the data is stored in a table. This is a format, that has been around for a while and it is perfectly fine to work with, however often it is more convenient to use a dataframe.\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column.\n\nThe column names should be non-empty.\nThe row names should be unique.\nThe data stored in a data frame can be of numeric, factor or character type.\nEach column should contain same number of data items.\n\ndataframes also should be tidy (Wickham 2014). But what is a tidy dataframe? There is a long philosophical debate, that is interesting to follow, but hold only little practical value. The ground rule for tidy data is:\n\n\n\n\n\n\nImportant\n\n\n\nEach column is a variable, each observation is a row.\n\n\nThis means, that sometimes data is repeated for the sake of clarity. For everyone who is new to this concept, this first sounds over the top, but once you get used to it, you will never want to look at data any other way.\nThis is also where the name tidyverse comes from. Once you stick to that fundamental design philosophy, the functions in the tidyverse are very simple to use.\nBack to the conversion, so … how?\n\n\n\n\n\n\n\n\nYou can see above the principle of tidy data.\nNow, the glimpse() command also give a different result.\n\n\n\n\n\n\n\n\nYou can also play around with the summary() function on the Titanic table or the Titanic dataframe.",
    "crumbs": [
      "Data Sources",
      "data shipped with R"
    ]
  }
]